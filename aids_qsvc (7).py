# -*- coding: utf-8 -*-
"""AIDS QSVC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_OPXESfhG5QIYs3MvMbZBZ70jKybeA1i
"""

!pip install qiskit==1.4.0
!pip install qiskit-algorithms==0.3.1
!pip install qiskit-aer
!pip install qiskit-machine-learning
!pip install pylatexenc

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, roc_auc_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from imblearn.over_sampling import SMOTE
from qiskit_aer.primitives import EstimatorV2, SamplerV2
from qiskit_aer import AerSimulator
from qiskit.circuit.library import ZZFeatureMap, TwoLocal, RealAmplitudes
from qiskit_machine_learning.algorithms.classifiers import QSVC
from qiskit_machine_learning.optimizers import COBYLA
from qiskit_algorithms.utils import algorithm_globals

mental_health_df = pd.read_csv("mental_health_dataset.csv")

# Display the first few rows
print(mental_health_df.head())

mental_health_df["Depression_Risk"] = (mental_health_df["PHQ9_Score"] > 10).astype(int)

# Define features and target
X = mental_health_df.drop(columns=["PHQ9_Score", "Depression_Risk", "Patient_ID"])
y = mental_health_df["Depression_Risk"]

from sklearn.preprocessing import StandardScaler, PolynomialFeatures, QuantileTransformer
from imblearn.combine import SMOTETomek
from sklearn.decomposition import PCA

poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
X_poly = poly.fit_transform(X)

selected_features = ['GAD7_Score', 'PTSD_Symptoms', 'Support_System', 'Sought_Therapy']
X_selected = mental_health_df[selected_features]

plt.figure(figsize=(10, 6))
sns.heatmap(X_selected.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation Heatmap After Selection')
plt.show()

scaler = QuantileTransformer(output_distribution='normal')
X_scaled = scaler.fit_transform(X_poly)

pca = PCA(n_components=0.99)
X_pca = pca.fit_transform(X_selected)

import seaborn as sns
import matplotlib.pyplot as plt

# Visualize class distribution before applying ADASYN
sns.countplot(x=y)
plt.title('Class Distribution Before ADASYN')
plt.show()

from imblearn.over_sampling import ADASYN
adasyn = ADASYN(random_state=42)
X_resampled, y_resampled = adasyn.fit_resample(X_pca, y)
sns.countplot(x=y_resampled)
plt.title('Class Distribution After ADASYN')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

from qiskit.circuit.library import ZZFeatureMap, PauliFeatureMap
from qiskit.primitives import Sampler
from qiskit_algorithms.state_fidelities import ComputeUncompute
from qiskit_machine_learning.kernels import FidelityQuantumKernel
from qiskit import QuantumCircuit
from qiskit.circuit import ParameterVector

def create_fm(num_features, fm_type, reps, entanglement):
    if not isinstance(num_features, int) or num_features <= 0:
        raise ValueError("num_features must be a positive integer.")
    if not isinstance(reps, int) or reps <= 0:
        raise ValueError("reps must be a positive integer.")
    if not isinstance(entanglement, (str, list)):
        raise ValueError("entanglement must be a string or a list of tuples.")

    if fm_type == 'ZZFeatureMap':
        fm = ZZFeatureMap(feature_dimension=num_features, reps=reps, entanglement=entanglement)
    elif fm_type == 'PauliFeatureMap':
        fm = PauliFeatureMap(feature_dimension=num_features, reps=reps, entanglement=entanglement, paulis=['Z', 'X', 'Y'])
    else:
        raise ValueError("Unsupported feature map type. Use 'ZZFeatureMap', 'PauliFeatureMap'.")

    return fm

def get_user_input():
    fm_type = input("Enter the feature map type (ZZFeatureMap, PauliFeatureMap): ").strip()
    reps = int(input("Enter the number of repetitions (reps): ").strip())
    entanglement = input("Enter the entanglement type (full, linear) or list of tuples (e.g., [(0,1),(1,2)]): ").strip()

    if entanglement.lower() in ['full', 'linear']:
        entanglement = entanglement.lower()
    else:
        try:
            entanglement = eval(entanglement)
            if not isinstance(entanglement, list):
                raise ValueError
        except:
            raise ValueError("Invalid format for entanglement. Use 'full', 'linear' or a list of tuples.")

    return fm_type, reps, entanglement

num_features = X.shape[1]

fm_type, reps, entanglement = get_user_input()

fm = create_fm(num_features, fm_type=fm_type, reps=reps, entanglement=entanglement)
print(fm)

fm.decompose().draw(output='mpl')

#Training a QML Model
from qiskit.primitives import Sampler
from qiskit_algorithms.state_fidelities import ComputeUncompute
from qiskit_machine_learning.kernels import FidelityQuantumKernel

num_features = X.shape[1]

sampler = Sampler()

fidelity = ComputeUncompute(sampler=sampler)

adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=fm)

"""Ensemble with RF"""

from sklearn.ensemble import VotingClassifier
from qiskit_machine_learning.algorithms import QSVC
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV

qsvc = QSVC(quantum_kernel=adhoc_kernel, probability=True)

# Combine QSVC with a classical Random Forest
classical_model = RandomForestClassifier(random_state=42)
ensemble = VotingClassifier(estimators=[
    ('qsvc', qsvc),
    ('rf', classical_model)
], voting='soft')
ensemble.fit(X_train, y_train)
y_pred = ensemble.predict(X_test)

# Generate performance metrics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, ensemble.predict_proba(X_test)[:, 1])

report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nClassification Report:\n", report)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Ensemble with RF")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get predicted probabilities for the positive class
y_prob = ensemble.predict_proba(X_test)[:, 1]

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

"""QSVC Normal"""

from qiskit_machine_learning.algorithms import QSVC
from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV

qsvc = QSVC(quantum_kernel=adhoc_kernel, probability=True)
qsvc.fit(X_train, y_train)

y_pred = qsvc.predict(X_test)

# Generate performance metrics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, qsvc.predict_proba(X_test)[:, 1])

label_encoder = LabelEncoder()
label_encoder.fit(y) # Assuming 'y' is your original target variable

# Now you can use label_encoder in classification_report
report = classification_report(y_test, y_pred, target_names=[str(cls) for cls in label_encoder.classes_])

print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nClassification Report:\n", report)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Tuned SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from qiskit_machine_learning.algorithms import QSVC
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, classification_report
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

# Fit the QSVC model
qsvc = QSVC(quantum_kernel=adhoc_kernel, probability=True)
qsvc.fit(X_train, y_train)

# Predict labels
y_pred = qsvc.predict(X_test)

# Predict probabilities for the positive class (label = 1)
y_prob = qsvc.predict_proba(X_test)[:, 1]

# Evaluation Metrics
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', label=f"QSVC (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - QSVC")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Optional: Label encoding for string class labels
label_encoder = LabelEncoder()
label_encoder.fit(y_train)  # Fitting on training labels

# Classification Report
report = classification_report(y_test, y_pred, target_names=[str(cls) for cls in label_encoder.classes_])

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("\nClassification Report:\n", report)

"""**Classical SVC**"""

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# Define parameter grid for SVM
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.1, 1],
    'kernel': ['rbf', 'linear']
}

# Stratified K-Fold cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    SVC(probability=True, random_state=42),
    param_grid,
    cv=cv,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

# Fit the model
grid_search.fit(X_train, y_train)

# Best estimator
best_svc_model = grid_search.best_estimator_

# Print best parameters
print(f"Best Parameters: {grid_search.best_params_}")

# Predictions
y_pred = best_svc_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"\nAccuracy (Tuned SVM): {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Tuned SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC AUC
y_prob = best_svc_model.predict_proba(X_test)[:, 1]
roc_auc = roc_auc_score(y_test, y_prob)
print(f"ROC AUC Score: {roc_auc:.4f}")

from sklearn.metrics import roc_curve, auc

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Tuned SVM")
plt.legend(loc="lower right")
plt.grid()
plt.show()

"""RF"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
import seaborn as sns
import matplotlib.pyplot as plt

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Grid Search with Cross-Validation
grid_search_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=cv,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

# Fit the model
grid_search_rf.fit(X_train, y_train)

# Best model
best_rf_model = grid_search_rf.best_estimator_
print(f"Best Parameters: {grid_search_rf.best_params_}")

# Predictions
y_pred_rf = best_rf_model.predict(X_test)

# Evaluation
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"\nAccuracy (Tuned Random Forest): {accuracy_rf:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Tuned Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC AUC
y_prob_rf = best_rf_model.predict_proba(X_test)[:, 1]
roc_auc_rf = roc_auc_score(y_test, y_prob_rf)
print(f"ROC AUC Score: {roc_auc_rf:.4f}")

from sklearn.metrics import roc_curve, auc

# Compute ROC curve and AUC
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f"Random Forest ROC (AUC = {roc_auc_rf:.2f})")
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Tuned Random Forest")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

"""XAI"""

!pip install lime

import shap
import matplotlib.pyplot as plt

# Ensure X_test is a DataFrame
if not hasattr(X_test, 'columns'):
    import pandas as pd
    X_test = pd.DataFrame(X_test, columns=['GAD7_Score', 'PTSD_Symptoms', 'Support_System'])

# 1. Create SHAP Explainer for Random Forest
explainer_shap = shap.TreeExplainer(best_rf_model)

# 2. Compute SHAP values
shap_values = explainer_shap.shap_values(X_test)

# 3. Print the feature names
print("Feature Names Used in SHAP Analysis:")
for i, col in enumerate(X_test.columns):
    print(f"{i+1}. {col}")

shap_values_class_1 = shap_values[:, :, 1]

shap.summary_plot(shap_values_class_1, X_test, plot_type="bar")  # bar plot
shap.summary_plot(shap_values_class_1, X_test)                   # beeswarm plot